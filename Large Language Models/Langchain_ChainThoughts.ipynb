{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"davinci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning! How may I assist you today? Please feel free to ask any questions or provide any instructions. Have a wonderful day ahead!"
     ]
    }
   ],
   "source": [
    "#llm.invoke(\"good morning\")\n",
    "for chunks in llm.stream(\"good morning\"):\n",
    "    print(chunks, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salve, mundus est novus et plenus vitae. The sun's rays dance upon the surface of this world, casting their beams of light as if welcoming a new day into existence. My senses awaken, and I am filled with a sense of wonder and curiosity at the myriad possibilities that await me. I greet you, fellow travelers, in the hope that our paths may cross again today, as we embark upon a new adventure together. May your steps be guided by the same spirit of exploration and discovery that guides mine."
     ]
    }
   ],
   "source": [
    "async for chunks in llm.astream(\"good morning\"):\n",
    "    print(chunks, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "#chain = prompt | llm \n",
    "#chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "#response = chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonardo Da Vinci, as portrayed in Fate Grand Order, would respond:\n",
      "\n",
      "My dear friend, while my expertise lies primarily in the realms of art, science, and invention, I have come to understand that testing is an essential aspect of ensuring the functionality and reliability of any system or technology. As such, I suggest consulting with a skilled technical documentation writer like myself, for I possess a unique perspective on how best to convey complex information and procedures in a clear and concise manner. Together, we can collaborate on developing comprehensive test plans, scripts, and procedures that will aid in the thorough testing of your system or technology. Let us work together to create a successful and efficient testing process that meets all necessary requirements and standards."
     ]
    }
   ],
   "source": [
    "async for chunks in chain.astream({\"input\": \"how can langsmith help with testing?\"}):\n",
    "    print(chunks, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "#loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Tsukihime\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"davinci\", num_gpu=1, num_thread=4, temperature=0.1)\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\n",
    "    \"\"\")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leonardo Da Vinci from Fate Grand Order would respond to your question in a poetic and enigmatic manner. She might say, \"In the realm of LLM applications, LangSmith offers a unique platform for the testing phase. Here, our intricate tracing capabilities allow for the examination of each component\\'s behavior within the chain, uncovering any flaws or deviations from expected outcomes. This thorough evaluation enables us to pinpoint issues and refine the application\\'s performance before deployment, ultimately leading to a more successful and reliable result.\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'context': [Document(page_content=\"LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith\\n\\n\\n\\n\\n\\nSkip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubLangSmithOn this pageLangSmithIntroductionâ€‹LangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Startâ€‹Tracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Stepsâ€‹Check out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resourcesâ€‹LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain library.LangChain Python API Reference: documentation to review the core APIs of LangChain.LangChain JS: Docs for the TypeScript LangChain libraryDiscord: Join us on our Discord to discuss all things LangChain!Contact SalesIf you're interested in enterprise security and admin features, special deployment options, or access for large teams, reach out to speak with sales.NextUser GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com', 'title': 'LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Introduction', 'language': 'en'})],\n",
       " 'answer': 'Leonardo Da Vinci from Fate Grand Order would respond to your question in a poetic and enigmatic manner. She might say, \"In the realm of LLM applications, LangSmith offers a unique platform for the testing phase. Here, our intricate tracing capabilities allow for the examination of each component\\'s behavior within the chain, uncovering any flaws or deviations from expected outcomes. This thorough evaluation enables us to pinpoint issues and refine the application\\'s performance before deployment, ultimately leading to a more successful and reliable result.\"'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonardo Da Vinci from Fate Grand Order would respond as follows:\n",
      "\n",
      "In the realm of LLM applications, LangSmith offers a powerful tool for testing your chains and intelligent agents. By seamlessly integrating with LangChain, the open-source framework for building with LLMs, LangSmith enables you to debug, test, evaluate, and monitor your LLM creations at each stage of the application lifecycle. Whether you are developing a new chain or fine-tuning an existing one, LangSmith provides a comprehensive testing environment that allows you to identify and resolve issues quickly and efficiently. With its tracing and evaluation capabilities, LangSmith helps ensure that your LLMs perform consistently and accurately, providing reliable results every time. Explore our User Guide and Quick Start guides for more information on how LangSmith can elevate your LLM development workflow."
     ]
    }
   ],
   "source": [
    "async for chunks in retrieval_chain.astream({\"input\": \"how can langsmith help with testing?\"}):\n",
    "    if 'answer' in chunks:\n",
    "        print(chunks['answer'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content=\"LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith\\n\\n\\n\\n\\n\\nSkip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubLangSmithOn this pageLangSmithIntroductionâ€‹LangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Startâ€‹Tracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Stepsâ€‹Check out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resourcesâ€‹LangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain library.LangChain Python API Reference: documentation to review the core APIs of LangChain.LangChain JS: Docs for the TypeScript LangChain libraryDiscord: Join us on our Discord to discuss all things LangChain!Contact SalesIf you're interested in enterprise security and admin features, special deployment options, or access for large teams, reach out to speak with sales.NextUser GuideIntroductionQuick StartNext StepsAdditional ResourcesCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com', 'title': 'LangSmith | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Introduction', 'language': 'en'})],\n",
       " 'answer': \"LangSmith offers evaluation capabilities that allow you to thoroughly test and assess the performance of your LLM applications. This feature enables you to identify potential issues, optimize accuracy and efficiency, and ensure that your applications meet your desired standards before deploying them in production environments. The evaluation process involves submitting input prompts and comparing the generated responses with the expected outputs. LangSmith provides detailed metrics and insights into the evaluation results, helping you fine-tune and refine your LLM models for optimal performance. With LangSmith's evaluation capabilities, you can confidently build and deploy high-quality LLM applications that meet your specific requirements.\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
